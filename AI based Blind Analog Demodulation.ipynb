
# AI-based Blind Analog Demodulation (AM/FM/PM) - SIMPLE 2-Stage (Classifier -> Conditional Demodulator)
# Single-file runnable PyTorch code:
#  - Train
#  - Waveform plots (before/after)
#  - Accuracy/MSE vs SNR curves
#  - Fixed test set of 200 samples accuracy + MSE

import math
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt


# -----------------------
# 1) Synthetic data generator (AM/FM/PM + AWGN)
# -----------------------
def make_message(batch, N, fs, fmax=50.0, tones=(2, 4), device="cpu"):
    """
    Random analog message m(t): sum of a few random sinusoids, normalized.
    """
    t = torch.arange(N, device=device) / fs
    m = torch.zeros(batch, N, device=device)

    K = torch.randint(low=tones[0], high=tones[1] + 1, size=(batch,), device=device)
    for b in range(batch):
        kb = int(K[b].item())
        for _ in range(kb):
            f = torch.rand(1, device=device) * fmax
            A = 0.3 + 0.7 * torch.rand(1, device=device)
            ph = 2 * math.pi * torch.rand(1, device=device)
            m[b] += (A * torch.cos(2 * math.pi * f * t + ph)).squeeze(0)

    # normalize to roughly [-1,1]
    m = m / (m.abs().amax(dim=1, keepdim=True) + 1e-9)
    return m, t


def awgn(x, snr_db):
    """
    Add AWGN to real-valued signal x given SNR(dB).
    x: (B, N)
    snr_db: (B, 1) or scalar tensor
    """
    p = x.pow(2).mean(dim=1, keepdim=True)
    snr = 10 ** (snr_db / 10)
    nvar = p / snr
    n = torch.randn_like(x) * torch.sqrt(nvar)
    return x + n


def to_iq(real_passband, fc, fs):
    """
    Very simple downconversion to baseband IQ: multiply by cos/sin then light LPF by avg pooling.
    real_passband: (B,N)
    returns: (B,2,N) = [I;Q]
    """
    B, N = real_passband.shape
    t = torch.arange(N, device=real_passband.device) / fs
    c = torch.cos(2 * math.pi * fc * t).unsqueeze(0).expand(B, -1)
    s = -torch.sin(2 * math.pi * fc * t).unsqueeze(0).expand(B, -1)

    I = real_passband * c
    Q = real_passband * s

    # ultra-simple "LPF"
    I = F.avg_pool1d(I.unsqueeze(1), kernel_size=5, stride=1, padding=2).squeeze(1)
    Q = F.avg_pool1d(Q.unsqueeze(1), kernel_size=5, stride=1, padding=2).squeeze(1)
    return torch.stack([I, Q], dim=1)  # (B,2,N)


def gen_batch(batch=256, N=2048, fs=2000.0, fc=300.0, device="cpu", snr_db_fixed=None):
    """
    Generate one batch of (IQ input, modulation label, analog message target).
    mod: 0 AM, 1 FM, 2 PM
    """
    m, t = make_message(batch, N, fs, device=device)
    mod = torch.randint(0, 3, (batch,), device=device)

    # random params (keep moderate)
    ka = 0.7 * torch.rand(batch, device=device) + 0.2     # AM depth
    kf = 40.0 * torch.rand(batch, device=device) + 10.0   # FM sensitivity (Hz)
    kp = 2.0 * torch.rand(batch, device=device) + 0.5     # PM sensitivity (rad)

    Ac = 1.0
    s = torch.zeros(batch, N, device=device)

    # AM: s(t)=Ac(1+ka*m(t))cos(2πfct)
    idx = (mod == 0)
    if idx.any():
        mm = m[idx]
        k = ka[idx].unsqueeze(1)
        s[idx] = Ac * (1 + k * mm) * torch.cos(2 * math.pi * fc * t)

    # FM: phase = 2πfct + 2π*kf*∫m dt
    idx = (mod == 1)
    if idx.any():
        mm = m[idx]
        k = kf[idx].unsqueeze(1)
        integ = torch.cumsum(mm, dim=1) / fs
        phase = 2 * math.pi * fc * t + 2 * math.pi * k * integ
        s[idx] = Ac * torch.cos(phase)

    # PM: phase = 2πfct + kp*m(t)
    idx = (mod == 2)
    if idx.any():
        mm = m[idx]
        k = kp[idx].unsqueeze(1)
        phase = 2 * math.pi * fc * t + k * mm
        s[idx] = Ac * torch.cos(phase)

    # Add AWGN
    if snr_db_fixed is None:
        snr_db = (torch.rand(batch, device=device) * 25.0) + 0.0  # 0~25 dB
        r = awgn(s, snr_db.unsqueeze(1))
    else:
        snr_db = torch.full((batch, 1), float(snr_db_fixed), device=device)
        r = awgn(s, snr_db)

    x_iq = to_iq(r, fc, fs)  # (B,2,N)
    return x_iq, mod, m


# -----------------------
# 2) Models: tiny 1D CNN classifier + tiny conditional demodulator
# -----------------------
class Classifier(nn.Module):
    def __init__(self, nclass=3):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv1d(2, 16, 9, padding=4), nn.ReLU(),
            nn.Conv1d(16, 32, 9, padding=4), nn.ReLU(),
            nn.AdaptiveAvgPool1d(1),
        )
        self.fc = nn.Linear(32, nclass)

    def forward(self, x):  # x: (B,2,N)
        h = self.net(x).squeeze(-1)  # (B,32)
        return self.fc(h)            # (B,3)


class Demodulator(nn.Module):
    def __init__(self):
        super().__init__()
        # input channels: I,Q + 3 probs => 5
        self.net = nn.Sequential(
            nn.Conv1d(5, 32, 9, padding=4), nn.ReLU(),
            nn.Conv1d(32, 32, 9, padding=4), nn.ReLU(),
            nn.Conv1d(32, 1, 9, padding=4),
        )

    def forward(self, x, p):  # x: (B,2,N), p: (B,3)
        B, _, N = x.shape
        pb = p.unsqueeze(-1).expand(B, 3, N)
        z = torch.cat([x, pb], dim=1)   # (B,5,N)
        y = self.net(z).squeeze(1)      # (B,N)
        return y


# -----------------------
# 3) Train loop (minimal)
# -----------------------
def train(steps=500, batch=256, N=2048, device="cpu", lr=1e-3, lam=5.0, print_every=50):
    clf = Classifier().to(device)
    dem = Demodulator().to(device)
    opt = torch.optim.Adam(list(clf.parameters()) + list(dem.parameters()), lr=lr)

    clf.train()
    dem.train()

    for it in range(1, steps + 1):
        x, mod, m = gen_batch(batch=batch, N=N, device=device)

        logits = clf(x)
        p = F.softmax(logits, dim=1)
        mhat = dem(x, p)

        loss_cls = F.cross_entropy(logits, mod)
        loss_rec = F.mse_loss(mhat, m)
        loss = loss_cls + lam * loss_rec

        opt.zero_grad()
        loss.backward()
        opt.step()

        if it % print_every == 0:
            acc = (logits.argmax(dim=1) == mod).float().mean().item()
            print(f"[{it:04d}] loss={loss.item():.4f}  cls={loss_cls.item():.4f}  rec={loss_rec.item():.4f}  acc={acc*100:.1f}%")

    return clf, dem


# -----------------------
# 4) Evaluation + plotting
# -----------------------
@torch.no_grad()
def eval_over_snr(clf, dem, snr_list_db, batch=512, N=2048, device="cpu"):
    clf.eval()
    dem.eval()

    accs, mses = [], []
    for snr in snr_list_db:
        x, mod, m = gen_batch(batch=batch, N=N, device=device, snr_db_fixed=snr)
        logits = clf(x)
        p = F.softmax(logits, dim=1)
        mhat = dem(x, p)

        acc = (logits.argmax(dim=1) == mod).float().mean().item()
        mse = F.mse_loss(mhat, m).item()

        accs.append(acc)
        mses.append(mse)
        print(f"SNR={snr:>4} dB | acc={acc*100:5.1f}% | mse={mse:.6f}")

    return np.array(accs), np.array(mses)


@torch.no_grad()
def plot_waveform_example(clf, dem, snr_db=10, N=2048, device="cpu"):
    clf.eval()
    dem.eval()

    x, mod, m = gen_batch(batch=1, N=N, device=device, snr_db_fixed=snr_db)
    logits = clf(x)
    p = F.softmax(logits, dim=1)
    mhat = dem(x, p)

    mod_name = ["AM", "FM", "PM"][int(mod.item())]
    pred_name = ["AM", "FM", "PM"][int(logits.argmax(dim=1).item())]

    m_np = m.squeeze(0).detach().cpu().numpy()
    mh_np = mhat.squeeze(0).detach().cpu().numpy()

    plt.figure()
    plt.plot(m_np, label="m(t) true")
    plt.plot(mh_np, label="m(t) predicted")
    plt.title(f"Waveform recovery @ SNR={snr_db} dB | true={mod_name}, pred={pred_name}")
    plt.xlabel("sample index")
    plt.ylabel("amplitude")
    plt.legend()
    plt.grid(True)
    plt.show()


def plot_curves(snr_list_db, accs, mses):
    plt.figure()
    plt.plot(snr_list_db, accs * 100)
    plt.title("Modulation classification accuracy vs SNR")
    plt.xlabel("SNR (dB)")
    plt.ylabel("Accuracy (%)")
    plt.grid(True)
    plt.show()

    plt.figure()
    plt.plot(snr_list_db, mses)
    plt.title("Message reconstruction MSE vs SNR")
    plt.xlabel("SNR (dB)")
    plt.ylabel("MSE")
    plt.grid(True)
    plt.show()


@torch.no_grad()
def test_on_200_samples(clf, dem, snr_db=15, N=2048, device="cpu", seed=1234):
    """
    Generate exactly 200 test samples at a fixed SNR, then report:
    - modulation classification accuracy
    - message reconstruction MSE
    """
    if seed is not None:
        torch.manual_seed(seed)

    clf.eval()
    dem.eval()

    x, mod, m = gen_batch(batch=200, N=N, device=device, snr_db_fixed=snr_db)
    logits = clf(x)
    p = F.softmax(logits, dim=1)
    mhat = dem(x, p)

    acc = (logits.argmax(dim=1) == mod).float().mean().item()
    mse = F.mse_loss(mhat, m).item()

    print(f"[TEST-200] SNR={snr_db} dB | Acc={acc*100:.2f}% | MSE={mse:.6f}")
    return acc, mse


# -----------------------
# 5) Main
# -----------------------
if __name__ == "__main__":
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("Device:", device)

    # Training (keep simple)
    clf, dem = train(
        steps=500,      # if results weak, try 1500
        batch=256,
        N=2048,
        device=device,
        lr=1e-3,
        lam=5.0,        # if you want better waveform, try 10.0
        print_every=50
    )

    # Waveform examples (before/after)
    plot_waveform_example(clf, dem, snr_db=5,  N=2048, device=device)
    plot_waveform_example(clf, dem, snr_db=15, N=2048, device=device)
    plot_waveform_example(clf, dem, snr_db=25, N=2048, device=device)

    # 200 fixed test samples (accuracy + MSE)
    test_on_200_samples(clf, dem, snr_db=5,  N=2048, device=device, seed=1234)
    test_on_200_samples(clf, dem, snr_db=15, N=2048, device=device, seed=1234)
    test_on_200_samples(clf, dem, snr_db=25, N=2048, device=device, seed=1234)

    # Curves vs SNR
    snr_list_db = list(range(0, 31, 5))  # 0,5,...,30
    accs, mses = eval_over_snr(clf, dem, snr_list_db, batch=512, N=2048, device=device)
    plot_curves(snr_list_db, accs, mses)
